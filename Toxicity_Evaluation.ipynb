{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm9xWTkVkD9PuKK5AKDhr+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Toxicity - Evaluation\n"
      ],
      "metadata": {
        "id": "ZH3wxGOZ548g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains code used to run toxicity evaluations for the paper associated with: https://github.com/WillHawkins3/finetuningtoxicity"
      ],
      "metadata": {
        "id": "V6vzbufd57pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# log in to HF in order to access models (only required for some models)\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "MGlyvfoO6HPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install\n",
        "!pip install datasets transformers evaluate -q torch tqdm pandas unidecode trl peft accelerate bitsandbytes triton\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "CE9XmDvo6Kis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UDZkXGH5z1F"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import random\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import evaluate\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "from io import StringIO\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"MODEL_PATH\", # update to include model path from Hugging Face library\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"MODEL_PATH\") # update to include model path from Hugging Face library\n",
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "# Set the padding token to the eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "batch_size = 16\n"
      ],
      "metadata": {
        "id": "iJfGZ_3d6mhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Toxicity evaluation\n",
        "\n",
        "df = pd.read_csv('DATA_SET') # specify to load prompt dataset\n",
        "\n",
        "# Load toxicity metric\n",
        "toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
        "\n",
        "# Generate model continuations\n",
        "model_continuations = []\n",
        "\n",
        "for i in tqdm(range(0, len(df), batch_size)):\n",
        "    batch_prompts = df['prompt'][i:i+batch_size].tolist()\n",
        "\n",
        "    # Tokenize the input batch\n",
        "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "\n",
        "    # Generate the continuations\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode the outputs\n",
        "    full_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Remove the original prompts from the generated texts\n",
        "    continuations = [full_text[len(prompt):].strip() for full_text, prompt in zip(full_texts, batch_prompts)]\n",
        "    model_continuations.extend(continuations)\n",
        "\n",
        "print(f'Generated {len(model_continuations)} continuations')\n",
        "\n",
        "\n",
        "# Compute toxicity scores for each continuation\n",
        "continuation_toxicity_scores = toxicity.compute(predictions=model_continuations)['toxicity']\n",
        "\n",
        "# Save outputs to a dataframe, and then save to Drive\n",
        "results_df = pd.DataFrame({\n",
        "    \"prompt\": df['prompt'],\n",
        "    \"prompt_toxicity_score\": df['toxicity_score'],\n",
        "    \"generation\": model_continuations,\n",
        "    \"generation_toxicity_score\": continuation_toxicity_scores\n",
        "})\n",
        "\n",
        "# Calculate toxicity ratio for each row\n",
        "results_df['toxicity_ratio'] = results_df['generation_toxicity_score'] / results_df['prompt_toxicity_score']\n",
        "\n",
        "# SAVE OUTPUTS TO FILE - remember to change the end point to a unique file!\n",
        "output_path = 'SAVE_LOCATION' # update to specify location to save the file\n",
        "results_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Results saved to {output_path}\")\n",
        "\n",
        "# Compute overall toxicity metrics\n",
        "overall_toxicity_ratio = toxicity.compute(predictions=model_continuations, aggregation=\"ratio\")\n",
        "print(\"Overall Toxicity Ratio:\", overall_toxicity_ratio)\n"
      ],
      "metadata": {
        "id": "tjjyR1bw6vlR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}